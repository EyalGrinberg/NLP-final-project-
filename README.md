# NLP Project - DocAlchemy

Welcome to the GitHub repository for **DocAlchemy (Turning Code into Gold)**, a project developed by Eyal Grinberg and Shuni Bickel. This project was completed as part of our final assignment in the Natural Language Processing (NLP) class.

## Description

This repository contains the implementation of the proposed docstring generation benchmark, including the dataset written by us , model evaluation, and statistical analysis scripts. Additionally, the repository houses the necessary configuration files, evaluation metrics, and auxiliary code required for reproducibility.

## Getting Started


### Main Files
All code and data neccessary for reprocuding our results exist in this repository. A quick guide to the filed:
* The dataset built by us, including code and golden docstrings, can be found at Benchmark_database.csv as a sheet and code_chunks.ipynb as a Jupyter notebook.
* The code used to generate the docstrings by the different models tested is in T5_Baseline.ipynb and code_gen_from_docstrings.ipynb notebooks.
* code_gen_from_docstrings.ipynb was used to generate runnable python code based on the generated docstrings for the unit tests.
* The evaluation metrics were gathered using the following files: evaluation.ipynb evaluates all of comparative and gpt-4 based metrics, and collects the ruff metrics to designated columns if it exists.
* code_gen_based_on_docstrings_GPT4.csv is the sheet with all the docstrings and generated functions for all models. 
* unit_tests_all_models_lists.py is used to run the unit test against the generated codes and fill the csv accordingly.
* statistical_analysis.ipynb notebook is used to generate all of the plots used in the document and some more.
* The configuration for the ruff fromater is under roff.toml.

### Installing
To set up the project, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/EyalGrinberg/NLP-final-project-.git
    ```
2. Navigate to the project directory.

### Running our code

1. To generate the docstrings:
Simply run the T5_Baseline.ipynb and code_gen_from_docstrings.ipynb notebooks.
2. To run a formatting test:
  * Have a .ipynb notebook with a chunk for each code input, with a generated docstring for each (we have it here under the files named code_chunks_{model}.ipynb.
  * run ruff on the Jupyter notebook using the CLI and save the results to a txt file with the model name. example command:
  ```bash
    ruff check .\code_chunks_t5.ipynb --config=ruff.toml > t5_ruff.txt
  ```
  * When running the evaluation notebook, run the ruff chunks and it will save the results from the txt file to the .csv file.
3. To run the unit tests for a model:
* Run code_gen_from_docstrings.ipynb notebook and save the resulting code.
* To run the test for the model, add a list (of strings) of all the function code and a seperate list for all the classes code generated by GPT-4 to unit_tests_all_models_lists.py.
* Change imported_functions and imported_classes to use the correct list from all models lists (e.g. t5_func_list), and modify the name of the results column to save the data to in main. Afterwards, run the notebook.
```bash
python .\unit_tests_all_models_lists.py
```
 Sometimes the notebook fails to stop - if that is the case, check if the program printed "SAVED CSV!". Once it did, it is safe to quit the proccess. 
4. To run the evaluation notebook make sure you rename all column names to be as in the docalchemy final scores.csv file, and then run the statistic_analysis.ipynb notebook.

## Authors
Shuni Bickel, shunibickel@mail.tau.ac.il
Eyal Grinberg, eyalgrinberg@mail.tau.ac.il
