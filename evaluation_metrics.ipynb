{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet rouge-score\n",
    "!pip install --upgrade --quiet nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use Unigrams, Bigrams and LCS. \n",
    "We will take the F1-score which combines the precison and recall as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(golden, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores_dict = scorer.score(golden, generated)\n",
    "    rouge1_f1 = scores_dict['rouge1'].fmeasure\n",
    "    rouge2_f1 = scores_dict['rouge2'].fmeasure\n",
    "    rougeL_f1 = scores_dict['rougeL'].fmeasure\n",
    "    return [rouge1_f1, rouge2_f1, rougeL_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use method5 as our smoothing funciton based on the results we've got when comparing to the other options and after reading 'A Systematic Comparison of Smoothing Techniques for Sentence-Level\n",
    "BLEU' by Boxing Chen and Colin Cherry.\n",
    "We've chosen it since it is quite intuitive and performs well for our purpouses - emphesizing meaning and recognizing the similarity between phrases even if there are slight variations or shifts in wording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def get_bleu_score(ref, candidate):\n",
    "    return sentence_bleu([ref.split()], candidate.split(), smoothing_function=chencherry.method5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generated docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927b05f602a54113a53a96532da5bc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f26e8ebc274b8d859dd48044e406ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3a516895ab4781a522d92476c4cb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaa2fb78c054ffca46c6976f285a8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "generated_docstrings_data = pd.read_csv(\"data_full_docstrings_generated.csv\")\n",
    "generated_docstrings_data.rename({'T5 BaseLine docstring generation': 'T5'}, axis=1, inplace=True)  # column name is too long\n",
    "\n",
    "models = ['T5', 'Gemini-1.0-pro', 'GPT-3.5 Turbo', 'Claude-instant-1']\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Add columns for metrics and initialize with None\n",
    "for model in models:\n",
    "    generated_docstrings_data[f'ROUGE-1 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-2 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-L f-score {model}'] = None\n",
    "    generated_docstrings_data[f'BLEU score {model}'] = None\n",
    "\n",
    "# A function to apply the evaluation functions\n",
    "def evaluate_row(row, model):\n",
    "    golden = row['Golden Docstring']\n",
    "    generated = row[model]\n",
    "    rouge_scores = evaluate_rouge(golden, generated)\n",
    "    bleu_score = get_bleu_score(golden, generated)\n",
    "    return pd.Series({\n",
    "        f'ROUGE-1 f-score {model}': rouge_scores[0],\n",
    "        f'ROUGE-2 f-score {model}': rouge_scores[1],\n",
    "        f'ROUGE-L f-score {model}': rouge_scores[2],\n",
    "        f'BLEU score {model}': bleu_score\n",
    "    })\n",
    "\n",
    "# Apply the function for each model and update the DataFrame\n",
    "for model in models:\n",
    "    mask = generated_docstrings_data[[f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}']].isna().any(axis=1)\n",
    "    generated_docstrings_data.loc[mask, [f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}']] = generated_docstrings_data.loc[mask].progress_apply(lambda row: evaluate_row(row, model), axis=1)\n",
    "\n",
    "generated_docstrings_data.to_csv(\"data_full_docs_gen_eval_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
