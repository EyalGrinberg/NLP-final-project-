{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet rouge-score\n",
    "!pip install --upgrade --quiet nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use Unigrams, Bigrams and LCS. \n",
    "We will take the F1-score which combines the precison and recall as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(golden, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores_dict = scorer.score(golden, generated)\n",
    "    rouge1_f1 = scores_dict['rouge1'].fmeasure\n",
    "    rouge2_f1 = scores_dict['rouge2'].fmeasure\n",
    "    rougeL_f1 = scores_dict['rougeL'].fmeasure\n",
    "    return [rouge1_f1, rouge2_f1, rougeL_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use method5 as our smoothing funciton based on the results we've got when comparing to the other options and after reading 'A Systematic Comparison of Smoothing Techniques for Sentence-Level\n",
    "BLEU' by Boxing Chen and Colin Cherry.\n",
    "We've chosen it since it is quite intuitive and performs well for our purpouses - emphesizing meaning and recognizing the similarity between phrases even if there are slight variations or shifts in wording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def get_bleu_score(ref, candidate):\n",
    "    return sentence_bleu([ref.split()], candidate.split(), smoothing_function=chencherry.method5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generated docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927b05f602a54113a53a96532da5bc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f26e8ebc274b8d859dd48044e406ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3a516895ab4781a522d92476c4cb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaa2fb78c054ffca46c6976f285a8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "generated_docstrings_data = pd.read_csv(\"data_full_docstrings_generated.csv\")\n",
    "generated_docstrings_data.rename({'T5 BaseLine docstring generation': 'T5'}, axis=1, inplace=True)  # column name is too long\n",
    "\n",
    "models = ['T5', 'Gemini-1.0-pro', 'GPT-3.5 Turbo', 'Claude-instant-1']\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Add columns for metrics and initialize with None\n",
    "for model in models:\n",
    "    generated_docstrings_data[f'ROUGE-1 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-2 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-L f-score {model}'] = None\n",
    "    generated_docstrings_data[f'BLEU score {model}'] = None\n",
    "\n",
    "# A function to apply the evaluation functions\n",
    "def evaluate_row(row, model):\n",
    "    golden = row['Golden Docstring']\n",
    "    generated = row[model]\n",
    "    rouge_scores = evaluate_rouge(golden, generated)\n",
    "    bleu_score = get_bleu_score(golden, generated)\n",
    "    return pd.Series({\n",
    "        f'ROUGE-1 f-score {model}': rouge_scores[0],\n",
    "        f'ROUGE-2 f-score {model}': rouge_scores[1],\n",
    "        f'ROUGE-L f-score {model}': rouge_scores[2],\n",
    "        f'BLEU score {model}': bleu_score\n",
    "    })\n",
    "\n",
    "# Apply the function for each model and update the DataFrame\n",
    "for model in models:\n",
    "    mask = generated_docstrings_data[[f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}']].isna().any(axis=1)\n",
    "    generated_docstrings_data.loc[mask, [f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}']] = generated_docstrings_data.loc[mask].progress_apply(lambda row: evaluate_row(row, model), axis=1)\n",
    "\n",
    "# generated_docstrings_data.to_csv(\"data_full_docs_gen_eval_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Function to process the text file and update the counts\n",
    "def process_ruff_output_text_file(file_name):\n",
    "    # Initialize a list of length 50 with zeros\n",
    "    counts = np.zeros(50, dtype=int)\n",
    "\n",
    "    try:\n",
    "        # Open the text file with the specified encoding\n",
    "        with open(file_name, 'r', encoding=\"utf-16\") as file:\n",
    "            # Read each line in the file\n",
    "            for line in file:\n",
    "                # Split the line into words\n",
    "                words = line.split(\" \")\n",
    "                # Check if the first word is a number between 1 and 50\n",
    "                if words and words[0] not in [\"Found\", \"All\"]:\n",
    "                    num = words[1].split(\":\")[0]\n",
    "                    if num.isdigit():\n",
    "                        # Increment the corresponding index in the list\n",
    "                        counts[int(num) - 1] += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error decoding file {file_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_mapping = {f'{model}' : '{model}_chunks.ipynb' for model in models}\n",
    "\n",
    "for model in model_names:\n",
    "    file_name = file_mapping.get(model)\n",
    "    if file_name:\n",
    "        counts = process_text_file(file_name)\n",
    "        generated_docstrings_data[f'ruff formatting {model}'] = counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_docstrings_data.to_csv(\"data_full_docs_gen_eval_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
