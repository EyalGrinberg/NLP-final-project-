{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai pillow\n",
    "\n",
    "!pip install --upgrade --quiet rouge-score\n",
    "!pip install --upgrade --quiet nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"hidden\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"hidden\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"hidden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You have vast knowledge of python functions and classes.\n",
    "You know and follow the google docstring conventions.\n",
    "You can evaluate the quality of a docstring when given a code snippet and a corresponding docstring.\n",
    "\"\"\"\n",
    "my_prompt = \"\"\"\n",
    "Given the following code: {code}\n",
    "and the following generated docstring: {generated_docstring}\n",
    "Assess the quality of the generated docstring based on the following criteria:\n",
    "For each evaluation metric, rate the quality of the generated docstring on a scale of 0 to 100.\n",
    "The evaluation metrics are:\n",
    "1. Accuracy - How well does the docstring describe the code?\n",
    "2. Completeness - Does the docstring contain all the necessary information about the code?\n",
    "3. Relevance - The ability of the docstring to stick to the point and not include irrelevant information.\n",
    "4. Understandability - How easy is it for a reader to understand the docstring?\n",
    "5. Readability - How well is the docstring formatted and structured?\n",
    "Your response shoulde be only a python list of evaluation metrics [<Accuracy score>, <Completeness score>, <Relevance score>, <Understandability score>, <Readability score>],\n",
    "do not include any other information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", my_prompt),\n",
    "])\n",
    "\n",
    "llm_gpt_turbo = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "chain_solution_gpt_turbo = prompt | llm_gpt_turbo | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_docstring(code, generated_docstring, chain_solution):\n",
    "    evaluation_response = chain_solution.invoke({\"code\": code, \"generated_docstring\": generated_docstring})\n",
    "    try: # try to convert the string to a list.\n",
    "        return ast.literal_eval(evaluation_response)\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"The model's response string is not a valid list representation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 docstring evaluation with GPT 3.5 Turbo before using GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_full_docstrings_generated.csv\")\n",
    "function_10_code = data.loc[10, 'Function']\n",
    "gpt_turbo_docstring_function_10 = data.loc[10, 'GPT-3.5 Turbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 80, 90, 75, 85]\n"
     ]
    }
   ],
   "source": [
    "generated_evaluation_list = evaluate_docstring(function_10_code, gpt_turbo_docstring_function_10, chain_solution_gpt_turbo)\n",
    "print(generated_evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 docstring evaluation with Gemini before using GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 90, 95, 85, 90]\n"
     ]
    }
   ],
   "source": [
    "prompt_for_gemini = ChatPromptTemplate.from_template(\n",
    "    system_prompt + my_prompt\n",
    ")\n",
    "\n",
    "gemini_docstring_function_10 = data.loc[10, 'Gemini-1.0-pro']\n",
    "\n",
    "llm_google = ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-1.0-pro\")\n",
    "chain_solution_gemini = prompt_for_gemini | llm_google | output_parser\n",
    "\n",
    "generated_evaluation_list = evaluate_docstring(function_10_code, gemini_docstring_function_10, chain_solution_gemini)\n",
    "print(generated_evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use Unigrams, Bigrams and LCS. \n",
    "We will take the F1-score which combines the precison and recall as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(golden, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores_dict = scorer.score(golden, generated)\n",
    "    rouge1_f1 = scores_dict['rouge1'].fmeasure\n",
    "    rouge2_f1 = scores_dict['rouge2'].fmeasure\n",
    "    rougeL_f1 = scores_dict['rougeL'].fmeasure\n",
    "    return [rouge1_f1, rouge2_f1, rougeL_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use method5 as our smoothing funciton based on the results we've got when comparing to the other options and after reading 'A Systematic Comparison of Smoothing Techniques for Sentence-Level\n",
    "BLEU' by Boxing Chen and Colin Cherry.\n",
    "We've chosen it since it is quite intuitive and performs well for our purpouses - emphesizing meaning and recognizing the similarity between phrases even if there are slight variations or shifts in wording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def get_bleu_score(ref, candidate):\n",
    "    return sentence_bleu([ref.split()], candidate.split(), smoothing_function=chencherry.method5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834db1f41ec941a8a43a258774c5a365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c1ae5d8d774819a32428b746abfaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0746ca5dfe89423492391bfb9445fa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ec547078554eef9096f454ce184ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0                                           Function  \\\n",
      "0  Recursion  def sum_even(lst): \\r\\n    total = 0\\r\\n    fo...   \n",
      "1        NaN  def find_num_changes(n, lst): \\n    if n == 0:...   \n",
      "2        NaN  def sum_nested(lst): \\n    if len(lst) == 0:\\n...   \n",
      "3        NaN  def str_decomp(target, word_bank): \\n    if ta...   \n",
      "4        NaN  def n_choose_k(n, k): \\n    if k < 0 or k > n:...   \n",
      "\n",
      "                                    Golden Docstring  \\\n",
      "0  Recursively compute the sum of all the element...   \n",
      "1  Recursively compute the number of ways to repr...   \n",
      "2  Recursively compute the absolute sum of all th...   \n",
      "3  Recursively compute the number of ways to deco...   \n",
      "4  Compute the number of options to choose k samp...   \n",
      "\n",
      "                                          Unit Tests  \\\n",
      "0  print(sum_even([1, [2, 3, [4, 5]]]) == 7)  \\np...   \n",
      "1  print(find_num_changes(4, [1, 2, 3]) == 4)\\npr...   \n",
      "2  print(sum_nested([1, 2, [3, 4], [5, [6, 7], 8]...   \n",
      "3  print(str_decomp(\"abcdef\", [\"ab\", \"abc\", \"cd\",...   \n",
      "4  print(n_choose_k(8, 8) == 1)\\r\\nprint(n_choose...   \n",
      "\n",
      "                                                  T5  \\\n",
      "0  Return the sum of the items that are delimited...   \n",
      "1                         Find the number of changes   \n",
      "2             Return the sum for nested structures .   \n",
      "3       Gets the total number of words in a string .   \n",
      "4  Return the number of k - columns and k - > val...   \n",
      "\n",
      "                                      Gemini-1.0-pro  \\\n",
      "0  Iteratively compute the sum of all even-indexe...   \n",
      "1  Iteratively compute the number of ways to make...   \n",
      "2  Iteratively compute the absolute sum of all el...   \n",
      "3  Iteratively compute the number of ways a targe...   \n",
      "4  Calculate the binomial coefficient of n and k,...   \n",
      "\n",
      "                                       GPT-3.5 Turbo  \\\n",
      "0  \\nCalculate the sum of even-indexed elements i...   \n",
      "1  \\nFind the number of ways to represent a given...   \n",
      "2  \\n    Recursively calculates the sum of all el...   \n",
      "3  \\nRecursively decomposes a target string using...   \n",
      "4  \\nCalculate the number of ways to choose k ele...   \n",
      "\n",
      "                                    Claude-instant-1 ROUGE-1 f-score T5  \\\n",
      "0   Computes the sum of all even indexed elements...           0.235294   \n",
      "1   Finds the number of ways coins can sum to n c...           0.101695   \n",
      "2   '''Iteratively compute the absolute nested su...           0.222222   \n",
      "3   \"\"\"Iteratively compute the total number of wa...           0.235294   \n",
      "4   Compute binomial coefficients (also called th...               0.28   \n",
      "\n",
      "  ROUGE-2 f-score T5  ... Readability GPT-3.5 Turbo  \\\n",
      "0           0.081633  ...                      95.0   \n",
      "1           0.070175  ...                      60.0   \n",
      "2                0.0  ...                      90.0   \n",
      "3           0.081633  ...                     100.0   \n",
      "4              0.125  ...                      80.0   \n",
      "\n",
      "  ROUGE-1 f-score Claude-instant-1 ROUGE-2 f-score Claude-instant-1  \\\n",
      "0                         0.534653                         0.222222   \n",
      "1                         0.344828                         0.174419   \n",
      "2                         0.705882                         0.545455   \n",
      "3                              0.6                         0.351852   \n",
      "4                         0.485437                         0.277228   \n",
      "\n",
      "  ROUGE-L f-score Claude-instant-1 BLEU score Claude-instant-1  \\\n",
      "0                          0.39604                    0.178698   \n",
      "1                         0.298851                    0.177765   \n",
      "2                         0.676471                    0.442592   \n",
      "3                         0.509091                    0.224402   \n",
      "4                         0.407767                    0.200008   \n",
      "\n",
      "  Accuracy Claude-instant-1 Completeness Claude-instant-1  \\\n",
      "0                      80.0                          80.0   \n",
      "1                      90.0                          90.0   \n",
      "2                      80.0                          90.0   \n",
      "3                     100.0                          80.0   \n",
      "4                      80.0                          90.0   \n",
      "\n",
      "  Relevance Claude-instant-1 Understandability Claude-instant-1  \\\n",
      "0                       90.0                               80.0   \n",
      "1                       80.0                               90.0   \n",
      "2                       90.0                               80.0   \n",
      "3                      100.0                              100.0   \n",
      "4                       90.0                               80.0   \n",
      "\n",
      "  Readability Claude-instant-1  \n",
      "0                         80.0  \n",
      "1                         90.0  \n",
      "2                         90.0  \n",
      "3                        100.0  \n",
      "4                         90.0  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prompt_for_gemini = ChatPromptTemplate.from_template(\n",
    "    system_prompt + my_prompt\n",
    ")\n",
    "\n",
    "llm_google = ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-1.0-pro\")\n",
    "chain_solution_gemini = prompt_for_gemini | llm_google | output_parser\n",
    "\"\"\"\n",
    "\n",
    "llm_gpt4 = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-1106-Preview\")\n",
    "output_parser = StrOutputParser()\n",
    "chain_gpt4 = prompt | llm_gpt4 | output_parser\n",
    "\n",
    "generated_docstrings_data = pd.read_csv(\"data_full_docstrings_generated.csv\")\n",
    "generated_docstrings_data.rename({'T5 BaseLine docstring generation': 'T5'}, axis=1, inplace=True)  # column name is too long\n",
    "\n",
    "models = ['T5', 'Gemini-1.0-pro', 'GPT-3.5 Turbo', 'Claude-instant-1']\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Add columns for metrics and initialize with None\n",
    "for model in models:\n",
    "    generated_docstrings_data[f'ROUGE-1 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-2 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-L f-score {model}'] = None\n",
    "    generated_docstrings_data[f'BLEU score {model}'] = None\n",
    "    generated_docstrings_data[f'Accuracy {model}'] = None\n",
    "    generated_docstrings_data[f'Completeness {model}'] = None\n",
    "    generated_docstrings_data[f'Relevance {model}'] = None\n",
    "    generated_docstrings_data[f'Understandability {model}'] = None\n",
    "    generated_docstrings_data[f'Readability {model}'] = None\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate_row(row, model):\n",
    "    golden = row['Golden Docstring']\n",
    "    generated = row[model]\n",
    "    rouge_scores = evaluate_rouge(golden, generated)\n",
    "    bleu_score = get_bleu_score(golden, generated)\n",
    "    accuracy, completeness, relevance, understandability, readability = evaluate_docstring(row['Function'], generated, chain_gpt4)\n",
    "    return pd.Series({\n",
    "        f'ROUGE-1 f-score {model}': rouge_scores[0],\n",
    "        f'ROUGE-2 f-score {model}': rouge_scores[1],\n",
    "        f'ROUGE-L f-score {model}': rouge_scores[2],\n",
    "        f'BLEU score {model}': bleu_score,\n",
    "        f'Accuracy {model}': accuracy,\n",
    "        f'Completeness {model}': completeness,\n",
    "        f'Relevance {model}': relevance,\n",
    "        f'Understandability {model}': understandability,\n",
    "        f'Readability {model}': readability\n",
    "    })\n",
    "\n",
    "# Apply the function to each model and update the DataFrame\n",
    "for model in models:\n",
    "    mask = generated_docstrings_data[[f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}', f'Accuracy {model}', f'Completeness {model}', f'Relevance {model}', f'Understandability {model}', f'Readability {model}']].isna().any(axis=1)\n",
    "    generated_docstrings_data.loc[mask, [f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}', f'Accuracy {model}', f'Completeness {model}', f'Relevance {model}', f'Understandability {model}', f'Readability {model}']] = generated_docstrings_data.loc[mask].progress_apply(lambda row: evaluate_row(row, model), axis=1)\n",
    "\n",
    "generated_docstrings_data.to_csv(\"data_full_eval_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
