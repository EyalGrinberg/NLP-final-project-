{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: C:\\Users\\user\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: C:\\Users\\user\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: C:\\Users\\user\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai pillow\n",
    "\n",
    "!pip install --upgrade --quiet rouge-score\n",
    "!pip install --upgrade --quiet nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"hidden\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"hidden\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"hidden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You have vast knowledge of python functions and classes.\n",
    "You know and follow the google docstring conventions.\n",
    "You can evaluate the quality of a docstring when given a code snippet and a corresponding docstring.\n",
    "\"\"\"\n",
    "my_prompt = \"\"\"\n",
    "Given the following code: {code}\n",
    "and the following generated docstring: {generated_docstring}\n",
    "Assess the quality of the generated docstring based on the following criteria:\n",
    "For each evaluation metric, rate the quality of the generated docstring on a scale of 0 to 100.\n",
    "The evaluation metrics are:\n",
    "1. Accuracy - How well does the docstring describe the code?\n",
    "2. Completeness - Does the docstring contain all the necessary information about the code?\n",
    "3. Relevance - The ability of the docstring to stick to the point and not include irrelevant information.\n",
    "4. Understandability - How easy is it for a reader to understand the docstring?\n",
    "5. Readability - How well is the docstring formatted and structured?\n",
    "Your response shoulde be only a python list of evaluation metrics [<Accuracy score>, <Completeness score>, <Relevance score>, <Understandability score>, <Readability score>],\n",
    "do not include any other information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", my_prompt),\n",
    "])\n",
    "\n",
    "llm_gpt_turbo = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "chain_solution_gpt_turbo = prompt | llm_gpt_turbo | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_docstring(code, generated_docstring, chain_solution):\n",
    "    evaluation_response = chain_solution.invoke({\"code\": code, \"generated_docstring\": generated_docstring})\n",
    "    try: # try to convert the string to a list.\n",
    "        return ast.literal_eval(evaluation_response)\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"The model's response string is not a valid list representation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 docstring evaluation with GPT 3.5 Turbo before using GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_full_docstrings_generated.csv\")\n",
    "function_10_code = data.loc[10, 'Function']\n",
    "gpt_turbo_docstring_function_10 = data.loc[10, 'GPT-3.5 Turbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 90, 90, 85, 85]\n"
     ]
    }
   ],
   "source": [
    "generated_evaluation_list = evaluate_docstring(function_10_code, gpt_turbo_docstring_function_10, chain_solution_gpt_turbo)\n",
    "print(generated_evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 docstring evaluation with Gemini before using GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 70, 90, 80, 90]\n"
     ]
    }
   ],
   "source": [
    "prompt_for_gemini = ChatPromptTemplate.from_template(\n",
    "    system_prompt + my_prompt\n",
    ")\n",
    "\n",
    "gemini_docstring_function_10 = data.loc[10, 'Gemini-1.0-pro']\n",
    "\n",
    "llm_google = ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-1.0-pro\")\n",
    "chain_solution_gemini = prompt_for_gemini | llm_google | output_parser\n",
    "\n",
    "generated_evaluation_list = evaluate_docstring(function_10_code, gemini_docstring_function_10, chain_solution_gemini)\n",
    "print(generated_evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use Unigrams, Bigrams and LCS. \n",
    "We will take the F1-score which combines the precison and recall as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(golden, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores_dict = scorer.score(golden, generated)\n",
    "    rouge1_f1 = scores_dict['rouge1'].fmeasure\n",
    "    rouge2_f1 = scores_dict['rouge2'].fmeasure\n",
    "    rougeL_f1 = scores_dict['rougeL'].fmeasure\n",
    "    return [rouge1_f1, rouge2_f1, rougeL_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use method5 as our smoothing funciton based on the results we've got when comparing to the other options and after reading 'A Systematic Comparison of Smoothing Techniques for Sentence-Level\n",
    "BLEU' by Boxing Chen and Colin Cherry.\n",
    "We've chosen it since it is quite intuitive and performs well for our purpouses - emphesizing meaning and recognizing the similarity between phrases even if there are slight variations or shifts in wording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def get_bleu_score(ref, candidate):\n",
    "    return sentence_bleu([ref.split()], candidate.split(), smoothing_function=chencherry.method5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation full pipeline\n",
    "before executing this code, we checked that BLEU and ROUGE metrics work good and generated evaluation scores with Gemini which is cheaper than GPT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33430646af7d492381eb32e6e453d4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421ccda2305d4abfbaa66529739106c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8587a5babd94db2a5a649223476469f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1426e026ca5047e5ad143d1c3bfa1891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prompt_for_gemini = ChatPromptTemplate.from_template(\n",
    "    system_prompt + my_prompt\n",
    ")\n",
    "\n",
    "llm_google = ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-1.0-pro\")\n",
    "chain_solution_gemini = prompt_for_gemini | llm_google | output_parser\n",
    "\"\"\"\n",
    "\n",
    "llm_gpt4 = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-1106-preview\")\n",
    "output_parser = StrOutputParser()\n",
    "chain_gpt4 = prompt | llm_gpt4 | output_parser\n",
    "\n",
    "generated_docstrings_data = pd.read_csv(\"data_full_docstrings_generated.csv\")\n",
    "\n",
    "models = ['T5', 'Gemini-1.0-pro', 'GPT-3.5 Turbo', 'Claude-instant-1']\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Add columns for metrics and initialize with None\n",
    "for model in models:\n",
    "    generated_docstrings_data[f'ROUGE-1 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-2 f-score {model}'] = None\n",
    "    generated_docstrings_data[f'ROUGE-L f-score {model}'] = None\n",
    "    generated_docstrings_data[f'BLEU score {model}'] = None\n",
    "    generated_docstrings_data[f'Accuracy {model}'] = None\n",
    "    generated_docstrings_data[f'Completeness {model}'] = None\n",
    "    generated_docstrings_data[f'Relevance {model}'] = None\n",
    "    generated_docstrings_data[f'Understandability {model}'] = None\n",
    "    generated_docstrings_data[f'Readability {model}'] = None\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate_row(row, model):\n",
    "    golden = row['Golden Docstring']\n",
    "    generated = row[model]\n",
    "    rouge_scores = evaluate_rouge(golden, generated)\n",
    "    bleu_score = get_bleu_score(golden, generated)\n",
    "    accuracy, completeness, relevance, understandability, readability = evaluate_docstring(row['Function'], generated, chain_gpt4)\n",
    "    return pd.Series({\n",
    "        f'ROUGE-1 f-score {model}': rouge_scores[0],\n",
    "        f'ROUGE-2 f-score {model}': rouge_scores[1],\n",
    "        f'ROUGE-L f-score {model}': rouge_scores[2],\n",
    "        f'BLEU score {model}': bleu_score,\n",
    "        f'Accuracy {model}': accuracy,\n",
    "        f'Completeness {model}': completeness,\n",
    "        f'Relevance {model}': relevance,\n",
    "        f'Understandability {model}': understandability,\n",
    "        f'Readability {model}': readability\n",
    "    })\n",
    "\n",
    "# Apply the function to each model and update the DataFrame\n",
    "for model in models:\n",
    "    mask = generated_docstrings_data[[f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}', f'Accuracy {model}', f'Completeness {model}', f'Relevance {model}', f'Understandability {model}', f'Readability {model}']].isna().any(axis=1)\n",
    "    generated_docstrings_data.loc[mask, [f'ROUGE-1 f-score {model}', f'ROUGE-2 f-score {model}', f'ROUGE-L f-score {model}', f'BLEU score {model}', f'Accuracy {model}', f'Completeness {model}', f'Relevance {model}', f'Understandability {model}', f'Readability {model}']] = generated_docstrings_data.loc[mask].progress_apply(lambda row: evaluate_row(row, model), axis=1)\n",
    "\n",
    "generated_docstrings_data.to_csv(\"data_full_docs_gen_eval_metrics_BLEU_ROUGE_GPT4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD RUFF HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
